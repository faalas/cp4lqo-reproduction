{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_path = \"2-merged-results\"\n",
    "ceb_file = os.path.join(folder_path, \"merged_ceb.csv\")\n",
    "other_file = os.path.join(folder_path, \"merged_other.csv\")\n",
    "\n",
    "def extract_last_column(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        last_col_name = df.columns[-1] \n",
    "        return df[last_col_name].tolist() \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "ceb_values = extract_last_column(ceb_file)\n",
    "job_values = extract_last_column(other_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getC(R,delta):\n",
    "    # print(\"delta = \",delta)\n",
    "    sortedR = sorted(R)\n",
    "    n = len(sortedR)\n",
    "    print(\"total_number: \",n)\n",
    "    q_hat_index = math.ceil(((n+1)*(1-delta)))\n",
    "    return sortedR[q_hat_index-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 0.09\n",
    "def g(x, e=E):\n",
    "    return max(0,x-e)\n",
    "\n",
    "def g_inverse(x, e=E):\n",
    "    # if (x < 1-e):\n",
    "    #     return x + e\n",
    "    # if x > 1-e:\n",
    "    #     return 1 \n",
    "    return min(1, x+e)\n",
    "\n",
    "def shift(c, K):\n",
    "    return g_inverse(g((1+1/K)*g_inverse(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceb_C = getC(ceb_values)\n",
    "print(\"ceb_C: \",ceb_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_C = getC(job_values)\n",
    "print(\"job_C: \",job_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustCP_one_iteration(job_values, ceb_values, delta, debug=True):\n",
    "    K =300 # Split Index\n",
    "\n",
    "    np.random.shuffle(job_values)\n",
    "\n",
    "    # JOB Calibration -> Sorted\n",
    "    job_calibration = job_values[:K] # K -> calibration data points\n",
    "    job_calibration = sorted(job_calibration)\n",
    "\n",
    "    # JOB Test -> Remaining values\n",
    "    job_test = job_values[K:] # N-K -> test data points\n",
    "\n",
    "    # CEB Test -> All the data points\n",
    "    job_calibration_C = getC(job_calibration, delta)\n",
    "    if debug: print(\"Original C: \", job_calibration_C)\n",
    "    \n",
    "    # Test on JOB\n",
    "    job_count = 0\n",
    "    for val in job_test:\n",
    "        if val<=job_calibration_C:\n",
    "            job_count+=1\n",
    "    job_test_valid_rate = job_count / len(job_test)\n",
    "    if debug: print(\"=> JOB Test: \",job_test_valid_rate)\n",
    "\n",
    "    robustCP_shift_quantile=shift(1-delta,K)\n",
    "    print(\"RobustCP_shift_quantile: \", robustCP_shift_quantile)\n",
    "    robustCP_index = math.ceil(((K+1)*robustCP_shift_quantile))\n",
    "    robustCP_index = min(robustCP_index, len(job_calibration))\n",
    "\n",
    "    robustCP_new_C = job_calibration[robustCP_index-1]\n",
    "    if debug: print(\"RobustCP_new_C: \",robustCP_new_C)\n",
    "\n",
    "    # Test on CEB\n",
    "    ceb_count = 0\n",
    "    for val in ceb_values:\n",
    "        if val < robustCP_new_C:\n",
    "            ceb_count +=1\n",
    "    ceb_test_valid_rate = ceb_count / len(ceb_values)\n",
    "    print(\"=> CEB Test: \",ceb_test_valid_rate)\n",
    "    return ceb_test_valid_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw the Quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "def drawing_robustCP_one_iteration(job_values, ceb_values, delta, debug=True):\n",
    "    K = 300 # Split Index\n",
    "\n",
    "    np.random.shuffle(job_values)\n",
    "\n",
    "    # JOB Calibration -> Sorted\n",
    "    job_calibration = job_values[:K] # K -> calibration data points\n",
    "\n",
    "    job_calibration = [i for i in job_calibration if i < 5000]\n",
    "    job_calibration = sorted(job_calibration)\n",
    "\n",
    "    # JOB Test -> Remaining values\n",
    "    job_test = job_values[K:] # N-K -> test data points\n",
    "\n",
    "    # CEB Test -> All the data points\n",
    "    job_calibration_C = getC(job_calibration, delta)\n",
    "    if debug: print(\"Original C: \", job_calibration_C)\n",
    "    \n",
    "    # Test on JOB\n",
    "    job_count = 0\n",
    "    for val in job_test:\n",
    "        if val<=job_calibration_C:\n",
    "            job_count+=1\n",
    "    job_test_valid_rate = job_count / len(job_test)\n",
    "    if debug: print(\"=> JOB Test: \",job_test_valid_rate)\n",
    "\n",
    "    robustCP_shift_quantile=shift(1-delta,K)\n",
    "    print(\"RobustCP_shift_quantile: \", robustCP_shift_quantile)\n",
    "    robustCP_index = math.ceil(((K+1)*robustCP_shift_quantile))\n",
    "    robustCP_index = min(robustCP_index, len(job_calibration))\n",
    "\n",
    "    robustCP_new_C = job_calibration[robustCP_index-1]\n",
    "    if debug: print(\"RobustCP_new_C: \",robustCP_new_C)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(job_calibration, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    plt.axvline(job_calibration_C, color='blue', linestyle='-', linewidth=2, label=r'Original C')\n",
    "    plt.axvline(robustCP_new_C, color='green', linestyle='-', linewidth=2, label=\"Adjusted $\\\\tilde{C}$ (Adaptive CP)\")\n",
    "    plt.xlabel(\"Nonconformity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title(\"Distribution of Nonconformity Score with $C$ and $\\\\tilde{C}$\")\n",
    "    plt.show()\n",
    "\n",
    "drawing_robustCP_one_iteration(job_values, ceb_values, delta = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_robustCP_one_iteration(job_values, ceb_values, delta = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_robustCP_one_iteration(job_values, ceb_values, delta = 0.2, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_robustCP_multiples_iterations(job_values, ceb_values, delta = 0.2, iterations=10000):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sys\n",
    "    import statistics\n",
    "    class HiddenPrints:\n",
    "        def __enter__(self):\n",
    "            self._original_stdout = sys.stdout\n",
    "            sys.stdout = open('/dev/null', 'w')  # macOS/Linux\n",
    "\n",
    "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "            \n",
    "            sys.stdout.close()\n",
    "            sys.stdout = self._original_stdout \n",
    "\n",
    "    ceb_test_valid_rates = []\n",
    "\n",
    "    with HiddenPrints():  \n",
    "        ceb_test_valid_rates = []\n",
    "        for i in range(iterations):\n",
    "            ans = test_robustCP_one_iteration(job_values, ceb_values, delta, debug = False)\n",
    "            ceb_test_valid_rates.append(ans)\n",
    "\n",
    "    avg_valid_rate = sum(ceb_test_valid_rates) / len(ceb_test_valid_rates) if ceb_test_valid_rates else 0\n",
    "    print(\"Average CEB Valid Rate:\", avg_valid_rate)\n",
    "    median_valid_rate = statistics.median(ceb_test_valid_rates) if ceb_test_valid_rates else 0\n",
    "    print(\"Median CEB Valid Rate:\", median_valid_rate)\n",
    "\n",
    "test_robustCP_multiples_iterations(job_values,ceb_values,delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare with the origianl C\n",
    "from scipy.stats import gaussian_kde\n",
    "def original_C_test_robustCP_one_iteration(job_values, ceb_values, delta, debug=True):\n",
    "    K = 300 # Split Index\n",
    "\n",
    "    np.random.shuffle(job_values)\n",
    "\n",
    "    # JOB Calibration -> Sorted\n",
    "    job_calibration = job_values[:K] # K -> calibration data points\n",
    "    job_calibration = sorted(job_calibration)\n",
    "\n",
    "    # CEB Test -> All the data points\n",
    "    job_calibration_C = getC(job_calibration, delta)\n",
    "\n",
    "    # Test on CEB\n",
    "    ceb_count = 0\n",
    "    for val in ceb_values:\n",
    "        if val < job_calibration_C:\n",
    "            ceb_count +=1\n",
    "    ceb_test_valid_rate = ceb_count / len(ceb_values)\n",
    "    print(\"=> CEB Test: \",ceb_test_valid_rate)\n",
    "    return ceb_test_valid_rate\n",
    "\n",
    "def original_C_test_robustCP_multiples_iterations(job_values, ceb_values, delta = 0.2, iterations=10000):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sys\n",
    "    import statistics\n",
    "    class HiddenPrints:\n",
    "        def __enter__(self):\n",
    "            self._original_stdout = sys.stdout\n",
    "            sys.stdout = open('/dev/null', 'w') \n",
    "\n",
    "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = self._original_stdout \n",
    "\n",
    "    ceb_test_valid_rates = []\n",
    "\n",
    "    with HiddenPrints(): \n",
    "        ceb_test_valid_rates = []\n",
    "        for i in range(iterations):\n",
    "            ans = original_C_test_robustCP_one_iteration(job_values, ceb_values, delta, debug = False)\n",
    "            ceb_test_valid_rates.append(ans)\n",
    "\n",
    "    avg_valid_rate = sum(ceb_test_valid_rates) / len(ceb_test_valid_rates) if ceb_test_valid_rates else 0\n",
    "    print(\"Average CEB Valid Rate:\", avg_valid_rate)\n",
    "    median_valid_rate = statistics.median(ceb_test_valid_rates) if ceb_test_valid_rates else 0\n",
    "    print(\"Median CEB Valid Rate:\", median_valid_rate)\n",
    "\n",
    "\n",
    "original_C_test_robustCP_multiples_iterations(job_values,ceb_values,delta=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_robustCP_multiples_iterations(job_values,ceb_values,delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_robustCP_multiples_iterations(job_values,ceb_values,delta=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_robustCP_multiples_iterations(job_values,ceb_values,delta=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
